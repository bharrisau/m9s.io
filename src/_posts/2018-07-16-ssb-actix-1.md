---
title: "Secure Scuttlebutt pub part 1: getting started"
---

I've been wanting to dig into both Actix, the Rust actor framework, and Secure Scuttlebutt, a new style of social network. This first part will be just be an opinionated starter project for an Actix production web service.

Let's start off by creating a project. In addition to Rust and Cargo, I have `hub` installed which is a CLI for accessing GitHub. We will also ensure some helper tools are installed.

```shell
$ cargo install cargo-edit cargo-watch systemfd
$ cd github.com/m9s
$ cargo new --vcs git --lib ssb-pub
$ cd ssb-pub
$ hub create m9s/ssb-pub
```

Now we will add some dependencies, I like to keep the library dependencies as light as possible and push all the initialisation components to the binary. At the moment Cargo doesn't support dependencies just for the binaries, but it does allow the 'dev-dependencies' to be just for tests and examples. So to keep our dependencies split I'll be pulling all the binary deps into the 'dev-dependencies' and calling `cargo run --example` to run our binary.

There is currently a lot of movement in the `tokio` and `futures` crates - I'm pretty sure current `futures` development is on the 0.3 branch but the latest crate is a 0.1 version. At the time of writing, only the git version of `actix-web` referenced the 0.7 `actix` which had started using the latest `tokio` - so you will see I use the git version below.

```shell
$ cargo add actix-web --git https://github.com/actix/actix-web.git
$ cargo add actix slog tokio futures serde serde_derive
$ cargo add --dev config listenfd slog-async slog-term slog-stdlog slog-scope slog-envlogger
```

I'm aiming to build this project with, what I understand to be, the current best practices for cloud native web services. That will include guidelines like [12-factor](https://12factor.net/) and support for [CNCF projects](https://www.cncf.io/projects/); e.g. health checks for Kubernetes, metrics for Prometheus, and tracing headers for OpenTracing. The source files may be trimmed for brevity, the full source of this part is available on [GitHub](https://github.com/m9s/ssb-pub/tree/5297b0ee32ba597b244489856767d30a48e314d6).

We will start by defining our configuration struct. We are pulling in `serde` here to handle generating the code to deserialise from TOML to our configuration struct. Because we will be running the web service along side the ssb pub service, let's split the configuration on those terms. We can either have the configuration defaults be in Rust or in TOML, I prefer to have the documentation and defaults live in the TOML. This slightly complicates tests because we need to reference the TOML when creating a configuration, but the `config` library makes it pretty easy.

```rust
// ./src/config.rs

#[derive(Debug, Deserialize)]
pub struct Config {
    pub web: Web,
}

#[derive(Debug, Deserialize)]
pub struct Web {
    pub motd: String,
    pub request_queue: usize,
}
```

```toml
# ./examples/default.toml
# Configuration defaults for ssb-pub

# Server section is used for startup of the server binary
[server]
# Specify the address and port to bind the server to. 
# This setting is ignored if a socket is passed in
# by environment (e.g. systemfd).
bind = "127.0.0.1:3123"

# etc
```

We can now implement our starter web service. This will eventually do useful things, but for now it is just an example of how to pass values through the system. In a simple `actix-web` project you'd be calling `bind(addr)` and `run()` on the server, in this example we are returning a `Sink` that accepts `TcpStream` because we will plan to multiplex a HTTP server on top of the same ssb port.

```rust
// ./src/web.rs

struct State {
    config: Arc<Config>,
    log: Logger,
}

fn index(req: HttpRequest<Arc<State>>) -> impl Responder {
    debug!(req.state().log, "index method called");
    format!("{}", req.state().config.web.motd)
}

pub fn start<S>(config: Arc<Config>, logger: &Logger) -> mpsc::Sender<S>
where S: AsyncRead + AsyncWrite + Send + 'static,
{
    let (tx, rx) = mpsc::channel(config.web.request_queue);
    let rx = rx.map_err(|_| io::ErrorKind::Other).from_err();
    let log = logger.new(o!("mod" => "web"));
    let state = Arc::new(State {
        config: config,
        log: log,
    });

    server::new(move || App::with_state(state.clone())
                .resource("/", |r| r.with(index)))
        .start_incoming(rx, false);

    tx
}
```

Now we can replace the default `src/lib.rs` with our version. Two things to note here, the first is that web service only requires a `AsyncRead + AsyncWrite` but we are requesting a `TcpStream`. We will be using the peek methods on `TcpStream` to determine where to send the stream so we specify it in the trait bound. The second is that most of this function is just mapping error values - this should hopefully become much cleaner with the 0.3/1.0 release of `futures`, but at the moment we are just going to live with this ugly code for the starter code.

```rust
// ./src/lib.rs

mod config;
mod web;

pub use config::Config;

pub fn start<S>(config: Config, logger: &Logger, stream: S)
where S: Stream<Item = TcpStream, Error = io::Error> + Sized + 'static
{
    let config = Arc::new(config);

    let web_app = web::start(config.clone(), logger)
        .sink_map_err(|_| ());

    let pipe = stream
        .map_err(|_| ())
        .forward(web_app)
        .map(|_| ());
    Arbiter::spawn(pipe);

    info!(logger, "Application started");
    info!(logger, "MOTD is {}", config.web.motd);
}
```

The code examples so far have been pretty tame, but as we left the binary to handle the messy instantiation these next parts are a little messier. First, we will read the configuration from our environment. We are using the `config` crate to handle merging the configuration layers and deserialising TOML where needed. We are using the `include_str!` macro to bundle the configuration defaults with the binary, then passing them as the first layer of config. We are also extracting the `Server` configuration without having mentioned it in the library.

```rust
// ./examples/server.rs

#[derive(Debug, Deserialize)]
struct Server {
    bind: String,
    log_level: String,
}

fn config() -> Result<(Server, ssb_pub::Config), ConfigError> {
    let mut s = Config::new();

    let default = include_str!("default.toml");
    s.merge(File::from_str(default, FileFormat::Toml))?;

    s.merge(File::with_name("config.toml").required(false))?;

    s.merge(Environment::with_prefix("app").separator("__"))?;

    let server: Server = s.get("server")?;

    s.try_into().map(|x| (server, x))
}
```

The `slog` instantiation is pretty by the book. We use the `slog_envlogger` to parse the log level configuration.

```rust
// ./examples/server.rs

fn logging(level: &str) -> Logger {
    let decorator = slog_term::TermDecorator::new().build();
    let drain = slog_term::FullFormat::new(decorator).build().fuse();
    let drain = slog_envlogger::LogBuilder::new(drain).parse(level).build().fuse();
    let drain = slog_async::Async::new(drain).build().fuse();

    slog::Logger::root(drain, slog_o!("version" => env!("CARGO_PKG_VERSION")))
}
```

The last requirement is the stream of connections. To allow quick reloading of the application during development, we are using the `systemfd/listenfd` crates. You can see more details on the [Actix website](https://actix.rs/docs/autoreload/). We need to have a `tokio` event loop running at this point to call `TcpListener::from_std`.

```rust
// ./examples/server.rs

fn listen(bind: &str) -> Incoming {
    let mut listenfd = ListenFd::from_env();

    if let Some(l) = listenfd.take_tcp_listener(0).unwrap() {
        info!("Listening on environment socket");
        TcpListener::from_std(l, &Handle::current()).unwrap().incoming()
    } else {
        info!("Listening on {}", bind);
        let addr = net::SocketAddr::from_str(bind).unwrap();
        TcpListener::bind(&addr).unwrap().incoming()
    }
}
```

Finally we have our main method. To ensure the async logger has a chance to flush all our logs we have wrapped the system in a function. When the `run()` function returns the logger references are dropped and they are flushed before we call `exit(code)`. We are also creating the `actix` system at the beginning of this block to ensure the `tokio` system is available when we create our TCP listener.

```rust
// ./examples/server.rs

fn run() -> i32 {
    let sys = actix::System::new("ssb-pub");

    let (server, config) = config().expect("Unable to parse config file");

    let logger = logging(&server.log_level);
    let _scope_guard = slog_scope::set_global_logger(logger);
    let _log_guard = slog_stdlog::init().unwrap();

    let connection_stream = listen(&server.bind);
    ssb_pub::start(config, &slog_scope::logger(), connection_stream);

    sys.run()
}

fn main() {
    let code = run();
    std::process::exit(code);
}
```

That brings us to the end of this first part. There is not yet any Secure Scuttlebutt code in here, but I feel that adding that will make this already long post an even tougher read. Again, the full source of this part is available [here](https://github.com/m9s/ssb-pub/tree/5297b0ee32ba597b244489856767d30a48e314d6) - I'm still learning the idioms of Rust so feel free to post some issues if there are parts I could have done better. I've also just knocked this blog together over a few weekends, so you can jump on the [blog repository](https://github.com/m9s/m9s.io) to complain about anything that is broken there.

In the next part we will implement some of the Secure Scuttlebutt protocol and show some examples of using the Rust/Cargo unit test tooling.
